{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02712fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f39a2",
   "metadata": {},
   "source": [
    "## Intro to RL\n",
    "\n",
    "### MDP Recap\n",
    "MDP = {State, Actions, Transition, Rewards}. The goal is to maximize the expected discounted return $\\mathbb{E}[\\sum_t \\gamma^tr_t]$.\n",
    "\n",
    "### Q-learning\n",
    "\"We learn an action-value table $Q(s,a)$ with the *off-policy* Bellman optimality update\"\n",
    "$$Q(s,a) \\leftarrow Q(s,a)(1 - \\alpha) + \\alpha \\left[ r + \\gamma \\max_{a'} \\left( Q(s', a') \\right) \\right]$$\n",
    "We use $\\epsilon$-greedy where we take the highest value action $\\epsilon$% of the time and a random exploratory action $1-\\epsilon$% of the time (actually we reversed it). Our hyperparameters are $\\alpha \\in (0,1], \\gamma \\in (0,1]$ which represent the learning rate and discount factor respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29fa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy after training: \n",
      "→ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\n",
      "→ → → ↓ ↓ ↓ ↓ ↓ ↓ ↓\n",
      "→ → → → ↓ ↓ → ↓ ↓ ↓\n",
      "→ → → → → → → ↓ ↓ ↓\n",
      "→ ↓ ↓ ↓ → → → ↓ ↓ ↓\n",
      "↓ ↓ ↓ → ↓ ↓ → → ↓ ↓\n",
      "→ → → → → → → → ↓ ↓\n",
      "→ → → → → → → → → G\n",
      "→ ↑ → ↑ → ↑ ↑ ↑ ↑ ↑\n",
      "→ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑\n",
      "Avg return (last 100):  -1.76\n"
     ]
    }
   ],
   "source": [
    "# ----- Gridworld -----\n",
    "class GridWorld:\n",
    "    def __init__(self, n=4, start=(0,0), goal=(3,3)):\n",
    "        self.n = n\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.state = start\n",
    "        self.actions = [0, 1, 2, 3] # up, right, down, left\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, a: int):\n",
    "        r, c = self.state\n",
    "\n",
    "        if a==0:\n",
    "            r = max(r - 1, 0)\n",
    "        elif a == 1:\n",
    "            c = min(c + 1, self.n - 1)\n",
    "        elif a == 2:\n",
    "            r = min(r + 1, self.n - 1)\n",
    "        elif a == 3:\n",
    "            c = max(c - 1, 0)\n",
    "        \n",
    "        next_state = (r, c)\n",
    "        done = next_state == self.goal\n",
    "        reward = 10.0 if done else -1\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def state_space(self) -> List[Tuple[int, int]]:\n",
    "        return [(r,c) for r in range(self.n) for c in range(self.n)]\n",
    "\n",
    "    def action_space(self) -> List[int]:\n",
    "        return self.actions\n",
    "\n",
    "\n",
    "# ----- Q-Learning ------\n",
    "def epsilon_greedy(Q, s, actions, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.choice(actions)\n",
    "\n",
    "    qs = [Q[(s, a)] for a in actions]\n",
    "    max_q = max(qs)\n",
    "    best = [a for a,q in zip(actions, qs) if q == max_q]\n",
    "    return random.choice(best)\n",
    "\n",
    "def train_q_learning(\n",
    "    env,\n",
    "    episodes=2000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    eps_start=1.0,\n",
    "    eps_end = 0.05,\n",
    "    eps_decay_episodes=1500\n",
    "):\n",
    "    Q = defaultdict(float)\n",
    "    actions = env.action_space()\n",
    "    returns = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        G = 0.0\n",
    "\n",
    "        # Linear epsilon decay\n",
    "        if eps_decay_episodes > 0:\n",
    "            eps = max(eps_end, eps_start - (eps_start - eps_end) * (ep / eps_decay_episodes))\n",
    "        else:\n",
    "            eps = eps_start # this seems wrong to me\n",
    "\n",
    "        while not done:\n",
    "            a = epsilon_greedy(Q, s, actions, eps)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            G += r\n",
    "\n",
    "            # Q-learning update\n",
    "            max_next = max(Q[(s_next, a2)] for a2 in actions) if not done else 0.0\n",
    "            td_target = r + gamma * max_next\n",
    "            Q[(s, a)] += alpha * (td_target - Q[(s, a)])\n",
    "\n",
    "            s = s_next\n",
    "        \n",
    "        returns.append(G)\n",
    "\n",
    "    return Q, returns\n",
    "\n",
    "# ----- Derive a greedy policy and visualize it -----\n",
    "ARROWS = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
    "\n",
    "def greedy_policy(Q, env):\n",
    "    pi = {}\n",
    "    for s in env.state_space():\n",
    "        if s == env.goal:\n",
    "            pi[s] = \"G\"\n",
    "            continue\n",
    "        best_a = max(env.action_space(), key=lambda a: Q[(s, a)]) # don't understand this\n",
    "        pi[s] = ARROWS[best_a]\n",
    "    return pi\n",
    "\n",
    "def print_policy(pi, n=4):\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            row.append(pi[(r, c)])\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# --- Test time ---\n",
    "env = GridWorld(n=10, start=(1,3), goal=(7,9))\n",
    "Q, returns = train_q_learning(env, episodes=3000, alpha=0.1, gamma=0.99)\n",
    "pi = greedy_policy(Q, env)\n",
    "print(\"Greedy policy after training: \")\n",
    "print_policy(pi, n=env.n)\n",
    "\n",
    "print(\"Avg return (last 100): \", sum(returns[-100:]) / 100.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrlenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

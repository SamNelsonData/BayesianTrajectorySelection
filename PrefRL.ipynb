{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a176ab",
   "metadata": {},
   "source": [
    "# Preference based Reinforcement Learning  \n",
    "Below are the equations describing the estimated probability according to the reward function that trajectory segment 1 will be preferable to trajectory segment 2 and the loss incurred based on that estimation and reality respectively. The last equation is the derivative for the loss function with respect to some parameter in the reward function.  \n",
    "$$P(\\sigma^1 > \\sigma^2) = \\frac{\\exp{(r(\\sigma^1))}}{\\exp{(r(\\sigma^1))} + \\exp{(r(\\sigma^2))}} = \\text{sigmoid}(r_1 - r_2)$$\n",
    "$$Loss(\\hat r_\\theta) = -\\sum_{\\sigma^1,\\sigma^2,\\mu} (1 - \\mu) \\ln{(P(\\sigma^1,\\sigma^2, \\hat r))} + \\mu\\ln{(P(\\sigma^2,\\sigma^1, \\hat r))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199b0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from numpy.random import normal, uniform, seed\n",
    "from numpy import sqrt, sum, abs, exp, where, array, zeros\n",
    "from math import factorial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "from RLHFPrefLib_old import CrossEntropyLoss\n",
    "\n",
    "from RLHFPrefLib import pref_estimation\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f178314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def print_chart(dictionary):\n",
    "    values = []\n",
    "    for key, value in dictionary.items():\n",
    "        print(f'{key:>20}', end=\"\")\n",
    "        values += [value]\n",
    "    print()\n",
    "    for row in zip(*values):\n",
    "        for column in row:\n",
    "            print(f'{column:>20.2f}', end=\"\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691c368",
   "metadata": {},
   "source": [
    "## Reward Function and Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82ff1b",
   "metadata": {},
   "source": [
    "## Basic Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd4d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reward1(state, action, slope=1e-1):\n",
    "    answer = state[0] + state[1]\n",
    "    distance = answer - action\n",
    "    y1 = 1 + slope*distance\n",
    "    y2 = 1 - slope*distance\n",
    "    return y1 if distance <=0 else y2\n",
    "\n",
    "def reward2(state, action):\n",
    "    answer = state[0] + state[1]\n",
    "    distance = answer - action\n",
    "    return (1 + torch.exp(-distance))**-1\n",
    "\n",
    "def reward(state, action, theta):\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=float)\n",
    "    r1 = theta[0]*reward1(state, action)\n",
    "    r2 = theta[1]*reward2(state, action)\n",
    "    return r1 + r2\n",
    "\n",
    "def max_action(state, reward_function, theta, init_action=0.0, lr=0.1, epochs=50):\n",
    "    action = torch.tensor(init_action, requires_grad=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        reward = reward_function(state, action, theta)\n",
    "        reward.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action += lr*action.grad\n",
    "            action.grad.zero_()\n",
    "    action.requires_grad_(False)\n",
    "    return action.detach()\n",
    "\n",
    "def evaluation(states, actions, reward, theta):\n",
    "    return torch.mean((torch.sum(states, axis=1) - actions)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0391c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: tensor([0., 0.], requires_grad=True)\n",
      "             Answers             Actions\n",
      "               12.00               10.00\n",
      "               12.00               10.00\n",
      "                2.00               10.00\n",
      "                8.00               10.00\n",
      "               13.00               10.00\n",
      "                9.00               10.00\n",
      "               10.00               10.00\n",
      "MSE Loss = 2.457\n",
      "             Answers             Actions\n",
      "               12.00               10.55\n",
      "               12.00               10.55\n",
      "                2.00                9.39\n",
      "                8.00                9.33\n",
      "               13.00               10.58\n",
      "                9.00                9.29\n",
      "               10.00               10.00\n",
      "MSE Loss = 1.902\n",
      "Parameters: tensor([1.2151, 0.0941], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Initialize Parameters ---\n",
    "theta = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "lr = 1.5e-1\n",
    "\n",
    "# --- Evaluate ---\n",
    "seed(42)\n",
    "states = torch.tensor(uniform(0, 10, size=(7, 2)).astype(int), requires_grad=False)\n",
    "\n",
    "greedy_actions = torch.tensor([max_action(s, reward, theta, init_action=10.0) for s in states])\n",
    "\n",
    "mse_loss = torch.mean((torch.sum(states, axis=1) - greedy_actions)**2) / 5\n",
    "\n",
    "print(f'Parameters: {theta}')\n",
    "print_chart({\n",
    "    'Answers': torch.sum(states, axis=1),\n",
    "    'Actions': greedy_actions\n",
    "})\n",
    "print(f'MSE Loss = {mse_loss:.3f}')\n",
    "\n",
    "theta.grad.zero_()\n",
    "\n",
    "\n",
    "# --- Train ---\n",
    "seed(60)\n",
    "epochs = 10 #200\n",
    "for epoch in range(epochs):\n",
    "    rand_range = exp(-(epoch - 300) / 500)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'  {int((epoch+1)*100 / epochs)}% complete')\n",
    "    # --- Set up States ---\n",
    "    states = torch.tensor(uniform(0, 10, size=(20, 2)).astype(int))\n",
    "    answers = torch.sum(states, axis=1)\n",
    "\n",
    "    # --- Find Actions ---\n",
    "    greedy_actions = torch.stack([max_action(s, reward, theta.detach(), init_action=10.0) for s in states]).detach()\n",
    "    random_actions = greedy_actions + torch.tensor(uniform(-rand_range, rand_range, size=len(states)))\n",
    "\n",
    "    # --- Get Preferences ---\n",
    "    prefs = (torch.abs(answers - greedy_actions) < torch.abs(answers - random_actions)).int()\n",
    "\n",
    "    # --- Calculate Loss ---\n",
    "    reward_pred = lambda state, action: reward(state, action, theta)\n",
    "    loss = CrossEntropyLoss(states, zip(greedy_actions, random_actions), prefs, reward_pred)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    gradient = theta.grad\n",
    "    with torch.no_grad():\n",
    "        theta += lr * gradient\n",
    "    \n",
    "    theta.grad.zero_()\n",
    "\n",
    "# --- Evaluate ---\n",
    "seed(42)\n",
    "states = torch.tensor(uniform(0, 10, size=(7, 2)).astype(int))\n",
    "greedy_actions = torch.tensor([max_action(s, reward, theta, init_action=10.0) for s in states])\n",
    "\n",
    "mse_loss = torch.mean((torch.sum(states, axis=1) - greedy_actions)**2) / 5\n",
    "\n",
    "print_chart({\n",
    "    'Answers': torch.sum(states, axis=1),\n",
    "    'Actions': greedy_actions\n",
    "})\n",
    "print(f'MSE Loss = {mse_loss:.3f}')\n",
    "\n",
    "theta.grad.zero_()\n",
    "print(f'Parameters: {theta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b6429",
   "metadata": {},
   "source": [
    "## NN Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664c8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNet(nn.Module):\n",
    "\n",
    "    def __init__(self, device = 'mps'):\n",
    "        super().__init__()\n",
    "        combos = factorial(3)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, combos), nn.LeakyReLU(),\n",
    "            nn.Linear(combos, combos), nn.LeakyReLU(),\n",
    "            nn.Linear(combos, 1)\n",
    "        )\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.tensor(x).float()\n",
    "        return self.net(x).flatten()\n",
    "\n",
    "    def action_max(self, x_init, lr=0.02, epochs=100):\n",
    "\n",
    "        if not torch.is_tensor(x_init):\n",
    "            x = torch.tensor(x_init, dtype=torch.float32, requires_grad=True)\n",
    "        else:\n",
    "            x = x_init.clone().detach()\n",
    "            x.requires_grad_(True)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            y = self.forward(x)\n",
    "            y.backward()\n",
    "            gradient = x.grad\n",
    "            with torch.no_grad():\n",
    "                addition = [0, 0, gradient[2]]\n",
    "                x += lr*torch.tensor(addition)\n",
    "            self.net.zero_grad()\n",
    "        return x[2]\n",
    "\n",
    "    def rand_weights(self, weight_var, bias_var, additive = False, seed = False):\n",
    "        state = self.state_dict()\n",
    "\n",
    "        if seed != False:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        weight_var = sqrt(weight_var)\n",
    "        bias_var = sqrt(bias_var)\n",
    "\n",
    "        for key in state.keys():\n",
    "            params = state[key]\n",
    "            center = 0.0 if not additive else params\n",
    "            if 'weight' in key and not torch.is_tensor(center):\n",
    "                state[key] = torch.normal(center, weight_var, size=params.shape)\n",
    "            elif 'bias' in key and not torch.is_tensor(center):\n",
    "                state[key] = torch.normal(center, bias_var, size=params.shape)\n",
    "            elif 'weight' in key and torch.is_tensor(center):\n",
    "                state[key] = torch.normal(center, weight_var)\n",
    "            elif 'bias' in key and torch.is_tensor(center):\n",
    "                state[key] = torch.normal(center, bias_var)\n",
    "                \n",
    "        self.load_state_dict(state)\n",
    "\n",
    "    def residual(self, x, y):\n",
    "        return y - self.forward(x)\n",
    "\n",
    "    def freeze_params(self):\n",
    "        flags = [p.requires_grad for p in self.parameters()]\n",
    "        for p in self.parameters: p.requires_grad_(False)\n",
    "        return flags\n",
    "    \n",
    "    def unfreeze_params(model, flags):\n",
    "        for p, f in zip(model.parameters(), flags): p.requires_grad_(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f12bd5",
   "metadata": {},
   "source": [
    "We're going to use `BCEWithLogitsLoss(reduction=\"sum\").(x, y)` which basically calculates:\n",
    "$$\\sum_{\\sigma_1 ... \\sigma_N}^N y_i*\\ln(\\sigma(x_i)) + (1 - y_i)*\\ln(1 - \\sigma(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a15248",
   "metadata": {},
   "source": [
    "# Test Training\n",
    "The training below adjusts the reward based on an optimal reward function and the loss compared to that. So far I have been having issues :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1392715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Distance    Predicted Reward       Actual Reward\n",
      "               26.00                1.21               -0.90\n",
      "               13.00               -0.56                1.05\n",
      "               27.00               -2.84               -1.05\n",
      "               10.00                0.89                1.50\n",
      "               19.00                1.72                0.15\n",
      "loss: 13.070858001708984\n",
      "            Distance    Predicted Reward       Actual Reward\n",
      "               26.00               -0.70               -0.90\n",
      "               13.00                1.41                1.05\n",
      "               27.00               -0.97               -1.05\n",
      "               10.00                1.50                1.50\n",
      "               19.00                0.12                0.15\n",
      "loss: 0.18015436828136444\n"
     ]
    }
   ],
   "source": [
    "# Test w/ MSELoss\n",
    "\n",
    "model = RewardNet()\n",
    "model.rand_weights(0.1, 0.1, seed=81)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "MSELoss = nn.MSELoss(reduction='sum')\n",
    "BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def true_reward(state, action, slope=15e-2, intercept=3):\n",
    "    answer = state[0] + state[1]\n",
    "    distance = answer - action\n",
    "    y1 = intercept + slope*distance\n",
    "    y2 = intercept - slope*distance\n",
    "    return y1 if distance <= 0 else y2\n",
    "\n",
    "# --- Init Evaluation ---\n",
    "model.eval()\n",
    "\n",
    "seed(42)\n",
    "states = torch.from_numpy(uniform(0, 20, size=(5,2)).astype(int)).float()\n",
    "actions = torch.from_numpy(uniform(0, 40, size=5).astype(int)).float()\n",
    "X = torch.column_stack((states, actions))\n",
    "\n",
    "y_pred = model.forward(X).flatten()\n",
    "y = torch.tensor([true_reward(s, a) for s,a in zip(states, actions)])\n",
    "\n",
    "\n",
    "print_chart({\n",
    "    'Distance': torch.abs(torch.sum(states, axis=1) - actions),\n",
    "    'Predicted Reward': y_pred,\n",
    "    'Actual Reward': y\n",
    "})\n",
    "\n",
    "print(f'loss: {MSELoss(y, y_pred)}')\n",
    "\n",
    "# --- Training ---\n",
    "model.train()\n",
    "seed(81)\n",
    "batch = 10\n",
    "for epoch in range(300):\n",
    "    \n",
    "    # --- Set up ---\n",
    "    states = torch.from_numpy(uniform(0, 20, size=(batch,2))).int().float()\n",
    "    actions = torch.from_numpy(uniform(0, 40, size=batch)).int().float()\n",
    "    y = torch.tensor([true_reward(s, a) for s,a in zip(states, actions)])\n",
    "    opt.zero_grad()\n",
    "    # --- Prediction --- \n",
    "    y_pred = model.forward(torch.column_stack((states, actions))).flatten()\n",
    "\n",
    "    # --- adjust model --- \n",
    "    loss = MSELoss(y, y_pred)\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "model.eval()\n",
    "\n",
    "seed(42)\n",
    "states = torch.from_numpy(uniform(0, 20, size=(5,2)).astype(int))\n",
    "actions = torch.from_numpy(uniform(0, 40, size=5).astype(int))\n",
    "X = torch.column_stack((states, actions)).float()\n",
    "\n",
    "y_pred = model.forward(X).flatten()\n",
    "y = torch.tensor([true_reward(s, a) for s,a in zip(states, actions)])\n",
    "\n",
    "print_chart({\n",
    "    'Distance': torch.abs(torch.sum(states, axis=1) - actions),\n",
    "    'Predicted Reward': y_pred,\n",
    "    'Actual Reward': y\n",
    "})\n",
    "print(f'loss: {MSELoss(y, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47afbb",
   "metadata": {},
   "source": [
    "# Future Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1085f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Answers      Greedy Actions      Random Actions          Real Prefs     Estimated Prefs\n",
      "               26.00               12.34               11.86                1.00                0.50\n",
      "               25.00               21.55               23.20                0.00                0.51\n",
      "                6.00                7.62                6.51                0.00                0.46\n",
      "               18.00                6.60                7.03                0.00                0.50\n",
      "               26.00               21.55               19.66                1.00                0.51\n",
      "Accuracy: 0.450\n",
      "             Answers      Greedy Actions      Random Actions          Real Prefs     Estimated Prefs\n",
      "               26.00               27.90               26.95                0.00                0.46\n",
      "               25.00               30.82               29.59                0.00                0.45\n",
      "                6.00                3.08                3.20                0.00                0.50\n",
      "               18.00               12.46               11.94                1.00                0.51\n",
      "               26.00               30.96               32.60                1.00                0.56\n",
      "Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "model = RewardNet()\n",
    "model.rand_weights(0.1, 0.1, seed=81)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "MSELoss = nn.MSELoss(reduction='sum')\n",
    "BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def true_reward(state, action, slope=15e-2, intercept=3):\n",
    "    answer = state[0] + state[1]\n",
    "    distance = answer - action\n",
    "    y1 = intercept + slope*distance\n",
    "    y2 = intercept - slope*distance\n",
    "    return y1 if distance <= 0 else y2\n",
    "\n",
    "model.eval()\n",
    "seed(42)\n",
    "\n",
    "test_size = 120\n",
    "with torch.no_grad():\n",
    "    states = torch.from_numpy(uniform(0, 20, size=(test_size,2)).astype(int)).float()\n",
    "answers = torch.sum(states, axis=1)\n",
    "\n",
    "greedy_actions = torch.tensor([model.action_max([s[0], s[1], 20.0]) for s in states])\n",
    "random_actions = greedy_actions + torch.empty(greedy_actions.size()).uniform_(-2, 2)\n",
    "\n",
    "X_greedy = torch.column_stack((states, greedy_actions))\n",
    "X_random = torch.column_stack((states, random_actions))\n",
    "\n",
    "r1 = model.forward(X_greedy)\n",
    "r2 = model.forward(X_random)\n",
    "\n",
    "prefs = (torch.abs(answers - greedy_actions) <= torch.abs(answers - random_actions)).float()\n",
    "\n",
    "est_prefs = pref_estimation((X_greedy, X_random), model.forward)\n",
    "\n",
    "accuracy = sum([torch.abs(p - ep) <= 0.5 for p, ep in zip(prefs, est_prefs)]) / test_size\n",
    "\n",
    "print_chart({\n",
    "    'Answers': answers[:5],\n",
    "    'Greedy Actions': greedy_actions[:5],\n",
    "    'Random Actions': random_actions[:5],\n",
    "    'Real Prefs': prefs[:5],\n",
    "    'Estimated Prefs': est_prefs[:5]\n",
    "})\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# --- training --- \n",
    "model.train()\n",
    "seed(81)\n",
    "batch=10\n",
    "loss_history = []\n",
    "for epoch in range(600):\n",
    "    dist = exp(-((epoch - 200) / 100)) + .2\n",
    "    # --- set up ---\n",
    "    with torch.no_grad():\n",
    "        states = torch.from_numpy(uniform(0, 20, size=(batch, 2))).int().float()\n",
    "    answers = torch.sum(states, axis=1)\n",
    "\n",
    "    # --- Action Calc ---\n",
    "    greedy_actions = torch.tensor([model.action_max([s[0], s[1], 20.0]) for s in states])\n",
    "    random_actions = greedy_actions + torch.empty(greedy_actions.size()).uniform_(-dist, dist)\n",
    "\n",
    "    # --- Reward Calc ---\n",
    "    X_greedy = torch.column_stack((states, greedy_actions))\n",
    "    X_random = torch.column_stack((states, random_actions))\n",
    "\n",
    "    r1 = model.forward(X_greedy)\n",
    "    r2 = model.forward(X_random)\n",
    "\n",
    "    # --- Calculate Loss + Update ---\n",
    "    prefs = (torch.abs(answers - greedy_actions) <= torch.abs(answers - random_actions)).float()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = BCELoss(r1 - r2, prefs)\n",
    "    loss_history += [loss]\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "seed(42)\n",
    "\n",
    "test_size = 300\n",
    "with torch.no_grad():\n",
    "    states = torch.from_numpy(uniform(0, 20, size=(test_size,2)).astype(int)).float()\n",
    "answers = torch.sum(states, axis=1)\n",
    "\n",
    "greedy_actions = torch.tensor([model.action_max([s[0], s[1], 20.0]) for s in states])\n",
    "random_actions = greedy_actions + torch.empty(greedy_actions.size()).uniform_(-2, 2)\n",
    "\n",
    "X_greedy = torch.column_stack((states, greedy_actions))\n",
    "X_random = torch.column_stack((states, random_actions))\n",
    "\n",
    "r1 = model.forward(X_greedy)\n",
    "r2 = model.forward(X_random)\n",
    "\n",
    "prefs = (torch.abs(answers - greedy_actions) <= torch.abs(answers - random_actions)).float()\n",
    "\n",
    "est_prefs = pref_estimation((X_greedy, X_random), model.forward)\n",
    "\n",
    "accuracy = sum([torch.abs(p - ep) <= 0.5 for p, ep in zip(prefs, est_prefs)]) / test_size\n",
    "\n",
    "print_chart({\n",
    "    'Answers': answers[:5],\n",
    "    'Greedy Actions': greedy_actions[:5],\n",
    "    'Random Actions': random_actions[:5],\n",
    "    'Real Prefs': prefs[:5],\n",
    "    'Estimated Prefs': est_prefs[:5]\n",
    "})\n",
    "print(f'Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d261cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = [100.0, 221.0]\n",
    "\n",
    "seg1 = state + [301.0]\n",
    "seg2 = state + [400.0]\n",
    "\n",
    "pref_estimation((seg1, seg2), model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8850d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrlenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
